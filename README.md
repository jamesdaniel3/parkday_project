# How To Run The Project

To build and run project in XCode:

```bash
npx expo run:ios
```

The backend is hosted on Google Cloud and all the API calls in the frontend are currently pointed to the hosted server, so you don't need to launch the server.

# Project Overview

For this project, I built the frontend in React-Native which I generated the boilerplate for with Expo. As requested, I built the backend in Node JS along with Koa and Knex.

All of the data currently in the production database was generated using the web scraper that was built as part of this project.

### Backend Design

A large focus of mine when building this project was creating a backend that I thought could be scalable and maintable. The only backend-related choice that I made to the contrary was writing the backend if JavaScript instead of TypeScript. While I am comfortable writing code in TypeScript, I was having too many issues with the config files when setting up the backend intially to justify the time sink that would come with contuing to use TypeScript. I opted to use JavaScript for the increased speed of developement, given the scope of the project.

The backend is hosted on Google Cloud Run and connected to a PostgreSQL instance in Google Cloud SQL. All of the API calls in the React-Native app and the scraper point to the production (hosted) endpoints. While this would not be the ideal choice for working in a project that is going to be released, it could easily be fixed by adding an environment variable which is used to set the base URL of the API calls. I chose to use the hosted endpoints because it removes the need to launch the backend when testing and I wanted to show that the backend could be used in a production environment, as I believe that setting up and deploying a server is a skill in itself.

When writing API endpoints for the first time or making migrations to the database, I would launch the server locally and first test the endpoints and the migrations against a local PostgreSQL instance.

The backend is containerized with Docker for the sake of deploying on Google Cloud. When I want to push updates to the Google Cloud deployed server, I run the bash script `gcloud_backend_deploy.sh`. When I want to run the server locally, I just run:

```bash
cd server
node server.js
```

The last notable feature of the backend was the use of migrations through Knex. After setting up connections to my produciton and development PostgreSQL instances, I needed to create the columns and tables of my database. I could have done this manually through a GUI or CLI for both instances, but I chose to set up migrations as I would want to have them on a larger project. They allow for version control within the database and provide a log of all changes made to the database over time.

To create a migration:

```bash
npm run make:migration -- <migration name>
```

After writing the migration file, you can run the migration with the following:

```bash
npm run migrate:dev
```

Then, the bash script will force the migraitons to be made when deploying the backend to google cloud.

### Database Design

![Database Diagram](/DatabaseDiagram.png)

Above is a diagram visualizing the structure of my database. The most notable departures from a basic database design for a project like this are the `operating_hours` and `menu_availabilty` tables. I built these tables to handle restaurants serving different menus on different days and times and to handle restaurants having different operating hours on different days. They add some complexity to the queries that need to be written and are probably not worth time the cost with the project at this scale, but I believe these tables or some other solution would be entirely necessary in a version of this project operating at a greater scale.

Not shown in the diagram are the knex_migrations and knex_migrations_lock tables that are generated by knex to maintain the version history of the tables. Additionally, every table has a column labeled created_at and a column labeled updated_at to keep track of the age of the data. This would be useful if we ever needed to do a role back or edit data from a certain period of time.

### Frontend Design

When designing the frontend, I wanted to integrate as many of the frames shown on the given Figma as possible. When looking to see how the frames might work together, I thought it made the most sense for the app to have two screens, at least to start.

The first screen holds all of the restaurants that are currently open. If you click on any of the restaurants, then all of the items that they are currently serving appear.

The UI is currently fairly simple, and I just focused on making the cleanest and most usable UI I could. There are many features that could be added which I will discuss below.

### Scraper Design

When I went to design the webscraper, I started off by testing many tools and seeing which I thought had potential. After using 3-4 tools marketed as AI-based web-scraping tools, I saw what appeared to be a fatal flaw. When encountering embedded images or PDFs, these tools just saw the tags and moved on, seemingly unable to extract the context from within the image or PDF that menus are so often located in.

To solve this problem, I went with the following process. The web scraper can be run from the command line and will ask users if the menu they want to scrape is on a website, or it's a PDF or image. If it's on a website, the script prompts Anthropic's Claude 3.7 Sonnet Model to read the website and return the data in a structured format (you can see the prompts in `prompts.py`). If the menu is a PDF or image, the user is asked to download/screenshot the PDF or image, and the text is extracted from the asset before being sent to the same model for structuring.

After the data is structured, a restaurant description is generated based on the data and the user is asked for any of the links that might be relevant to the restaurant (Resy, Instagram, etc...).

There is a decent amount of user input needed for this solution, but some of it can be removed in the future and there is no need for additional code writing.

I chose to write this functionality in Python because it had the tools I was most familair with for taking text from images or converting PDFs to plain text.

You can run the scraper with the following (on mac):

```bash
cd scraper
pip install -r requirements.txt
brew install tesseract
brew install poppler
python get_data.py
```

Depending on the value of `BASE_URL` in your environment variables, it will either update the production database or your local database.

# Future Roadmap

If I were to continue devleoping this project, here are a few of the changes I would like to make/features I would like to add.

### Searchability

I would like to add a search bar on both the homescreen and a menu screen to allow users to more effectively find restaurants or meals that they want.

### Admin Dashboard for Adding Website Info

I was not able to find an AI tool that was super effective at scraping the many different websites structures that different restaurants use. I think the best solution would be to build out a dashboard where a user can provide a link to the restaurant's website along with either a link to their menu or a file containing their menu. AI tools can then be used to generate as much information as possible about the restaurant. Once the information is generated, it could be displayed and users could edit anything they see fit. Finally, users could hit a button to confirm and the data could be sent to the database.

I built out most of this logic in my scraper, but I would definitely like to build a UI for it to minimize the technical barrier-to-entry for users.

### Improved Image Handling

Currently, the scraper is not able to extract images from menus and does not prompt the user for restaurant logos or storefront images. I didn't want to add this information to the prompts in the scraper as I wanted to minimize the load on the user to input a new website's data. I would like to either improve the scraper or find some other solution to handle image retrieval in order to improve the data available to the project.

### Improve Creation APIs

The APIs for adding restaurant data and menu data currently do not have functionality to update the operating hours table for a restaurant or the menu_availabilty table for a menu. The scraper would need to be improve to have this data, but it is necessary for the app to function if the scale were to continue to grow.

The search APIs can already effectively return restaurants that are open and menu items that are available, they just default to assuming availability if information is not available in the database to prove otherwise. I would like to improve the scraper and the APIs to allow for this data to be retreived.

### Checkout

I would like to add a checkout feature, as referenced in Figma by the "Reserve this lunch" Frame. This is quite a big feature and if it were to be integrated fully, we would have a working app where users could order food.

### Data Removal

There are not currently any API endpoints for removing data from the database. This would no-doubt be necessary as the project goes, but I wrote endpoints as I needed them when making this project and the issue did not arise with this scale.
